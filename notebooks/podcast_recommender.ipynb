{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install stop-words\n",
        "!pip install pyspark\n",
        "!pip install urllib3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qAueAHoBAf_",
        "outputId": "2b7c58e4-a834-406f-8e8b-392e2bcb0c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stop-words\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32911 sha256=8cd167ab61550322ac188420086442feff63153fbf07c47d72702be2ab8ef8c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/86/b2/277b10b1ce9f73ce15059bf6975d4547cc4ec3feeb651978e9\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 61 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 66.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=659d7cddf4efd33b8993d51ffd1706d1e43339153dacce04d5e4cff9940f51f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import itertools\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib3\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from stop_words import get_stop_words\n",
        "\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6GlFcQu-ZXT",
        "outputId": "24c474c3-ae10-41a7-a0ea-901ee3a5956a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Developing a Podcast Recommender!\n",
        "\n",
        "Final Project - APAM Senior Seminar - Fall 2022 - October 10\n",
        "\n",
        "Yamini Ananth, Jafar Vohra, Kathy Wang, Abhiram Kolluri"
      ],
      "metadata": {
        "id": "r6wvcIiGiRIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-Processing"
      ],
      "metadata": {
        "id": "z_rzY1P_-ZXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data scraped from Apple Podcasts using BeautifulSoup\n",
        "\n",
        "Scripts for web scraping attributed to [Siddharth Kumaran](https://github.com/siddgood/podcast-recommendation-engine/blob/master/scripts/get_podcast_info.py)\n",
        "\n",
        "Scraped data includes: \n",
        "* Title (text)\n",
        "* Producer (text)\n",
        "* Description (text)\n",
        "* 6 Recent Episode Titles (text)\n",
        "* 6 Recent Episode Descriptions (text)\n",
        "\n",
        "Pre-processing included:\n",
        "* Filtered out URLs and special characters\n",
        "* Tokenized (separated each word into its own string)\n",
        "* Removed stop-words (common words like articles, pronouns etc)\n",
        "* Lemmatized (removed endings from words, so ‘like’ and ‘likes’ and ‘likely’ would all be converted to ‘lik’)\n"
      ],
      "metadata": {
        "id": "lZg48Z_Rin_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Trying to get pickle data from http\n",
        "\n",
        "http = urllib3.PoolManager()\n",
        "req = http.request(\"GET\", \"https://github.com/yaminivibha/podcast-recs/blob/main/data/data/pickle_files/english_podcasts_detailed_cleaned.pkl\")\n",
        "podcast_data_pkl = req.data.decode('utf-8')\n",
        "#podcasts_df_orig = pd.read_pickle(podcast_data_pkl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEXz2YJhcQH8",
        "outputId": "7204f510-a9ae-49bc-c288-05e2f347cf33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "podcasts_df_orig = pd.read_pickle('/media/english_podcasts_detailed_cleaned.pkl')"
      ],
      "outputs": [],
      "metadata": {
        "id": "bjIzSS-3-ZXX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Combining all text data into one column for downstream analysis\n",
        "\n",
        "podcasts_df = podcasts_df_orig\n",
        "podcasts_df['text'] = podcasts_df[['title', 'producer', 'genre', 'description', 'episode_titles', 'episode_descriptions']].apply(lambda x: ' '.join(x), axis=1)\n",
        "podcasts_df = podcasts_df.drop(columns=['genre', 'description', 'num_episodes', 'rating', 'num_reviews', 'link', 'episode_titles', 'episode_descriptions'])\n",
        "podcasts_df['idx'] = list(range(podcasts_df.shape[0]))"
      ],
      "outputs": [],
      "metadata": {
        "id": "toiWYTPd-ZXY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Creating stopwords list & tokenizer\n",
        "\n",
        "stop = get_stop_words('en')\n",
        "stop = [re.sub(r'([^\\s\\w]|_)+', '', x) for x in stop]\n",
        "tokenizer = RegexpTokenizer(r'\\w+')"
      ],
      "outputs": [],
      "metadata": {
        "id": "zq5qlO4--ZXY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Creating helper functions to remove stop words \n",
        "# and lemmatize tokenized sentences\n",
        "\n",
        "def remove_stop(text, stop):\n",
        "    return [word for word in text if word not in stop ]\n",
        "\n",
        "def lemmatize(text, l_stemmer):\n",
        "    return [l_stemmer.lemmatize(word) for word in text]"
      ],
      "outputs": [],
      "metadata": {
        "id": "M5ZyzMby-ZXZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def preprocess_text(text):\n",
        "    # remove mixed alphanumeric, URLS, stop words\n",
        "    text = re.sub(r\"\"\"(?x) \\b(?=\\w*\\d)\\w+\\s*\"\"\",\"\", text)\n",
        "    re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r'([^\\s\\w]|_)+', '', text)\n",
        "    text = tokenizer.tokenize(text.lower())\n",
        "    text = remove_stop(text, stop)\n",
        "    text = lemmatize(text, WordNetLemmatizer())\n",
        "    \n",
        "    new_text = ' '.join(text)\n",
        "    return new_text"
      ],
      "outputs": [],
      "metadata": {
        "id": "mFXFWqCG-ZXZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "podcasts_df['text'] = podcasts_df['text'].map(preprocess_text)\n",
        "podcasts_df = podcasts_df.query('text !=\"\"')"
      ],
      "outputs": [],
      "metadata": {
        "id": "wmihuN4m-ZXa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Utilities for Recommendation\n",
        "\n",
        "Collection of helper functions"
      ],
      "metadata": {
        "id": "4zvnwusX-ZXa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_title_from_index(index):\n",
        "    \"\"\"get title of podcast from index of podcast\n",
        "        parameters:\n",
        "            index: (int)\n",
        "        returns:\n",
        "            title (string)\n",
        "        raises:\n",
        "            ValueError: index not in podcasts_df['idx']\n",
        "    \"\"\"\n",
        "    return podcasts_df[podcasts_df.idx == index][\"title\"].values[0]\n",
        "\n",
        "def get_index_from_title(title):\n",
        "    \"\"\"get index of podcast from title of podcast\n",
        "        parameters:\n",
        "            title: (string)\n",
        "        returns:\n",
        "            index (int)\n",
        "        raises:\n",
        "            ValueError: string not in podcasts_df['title']\n",
        "    \"\"\"\n",
        "    return podcasts_df[podcasts_df.title == title][\"idx\"].values[0]"
      ],
      "outputs": [],
      "metadata": {
        "id": "7A3fQVdT-ZXa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def recommend(podcast_title, sim_matrix, number_recs=5, pretty_print=True):\n",
        "    \"\"\"given a podcast title & a similarity matrix, return n most similar podcasts\n",
        "        parameters:\n",
        "            podcast_title: (str) must be in podcasts_tf['title]\n",
        "            sim_matrix: (np.array) similarity matrix\n",
        "            number_recs: (int) how many recommendations do you want per title?\n",
        "        returns:\n",
        "            recommendations: (list[str]) list of n most similar podcasts \n",
        "                            according to the similarity matrix\n",
        "    \"\"\"\n",
        "\n",
        "    podcast_id = get_index_from_title(podcast_title)\n",
        "    similar_podcasts =  list(enumerate(sim_matrix[podcast_id]))\n",
        "    sorted_similar_podcast = sorted(similar_podcasts,key=lambda x:x[1],reverse=True)\n",
        "    \n",
        "    recommendations = [get_title_from_index(sorted_similar_podcast[i][0]) for i in range(number_recs+2)]\n",
        "    \n",
        "    ### formatting for pretty printing ###\n",
        "    if pretty_print:\n",
        "      print(\"If you liked {}, try: \".format(podcast_title))\n",
        "      for i in recommendations[1:]:\n",
        "          print(\"     {}\".format(i))\n",
        "    \n",
        "    return recommendations[1:]"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZD7LRGxe-ZXb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Podcasts we'll use to validate results\n",
        "sample_podcasts = ['The Daily', \"Murder, etc.\",'This American Life', 'Call Her Daddy', 'The Joe Rogan Experience']"
      ],
      "outputs": [],
      "metadata": {
        "id": "hdMzyo51-ZXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words + Cosine Similarity"
      ],
      "metadata": {
        "id": "96uz3ukA-ZXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we use the bag of words model to encode the podcast text and use that to generate a cosine similarity matrix."
      ],
      "metadata": {
        "id": "Z1RMswK0mQG2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "cv = CountVectorizer()\n",
        "cv_matrix = cv.fit_transform(podcasts_df[\"text\"])\n",
        "cv_cosine_sim = cosine_similarity(cv_matrix)"
      ],
      "outputs": [],
      "metadata": {
        "id": "iOqY0hXX-ZXd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for i in sample_podcasts:\n",
        "    recs = recommend(i, cv_cosine_sim)\n",
        "    print('\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you liked The Daily, try: \n",
            "     Impeachment Inquiry: Updates from The Washington Post\n",
            "     Impeachment: A Daily Podcast\n",
            "     The Takeaway\n",
            "     Article II: Inside Impeachment\n",
            "     The Daily 202's Big Idea\n",
            "     The 11th Hour with Brian Williams\n",
            "\n",
            "\n",
            "If you liked Murder, etc., try: \n",
            "     Criminology\n",
            "     Murderville\n",
            "     Unsolved Murders: True Crime Stories\n",
            "     Murder Minute\n",
            "     Don't Talk to Strangers\n",
            "     True Crime All The Time Unsolved\n",
            "\n",
            "\n",
            "If you liked This American Life, try: \n",
            "     The Stoop Storytelling Series\n",
            "     The Story Home Children's Audio Stories\n",
            "     Spooky Boo's Scary Story Time\n",
            "     The Story Behind\n",
            "     This is the Gospel Podcast\n",
            "     1001 Heroes, Legends, Histories & Mysteries Podcast\n",
            "\n",
            "\n",
            "If you liked Call Her Daddy, try: \n",
            "     Stiff Socks\n",
            "     Two Judgey Girls\n",
            "     NAKED with Catt Sadler\n",
            "     Slay Girl Slay\n",
            "     Hot Marriage. Cool Parents.\n",
            "     Safe For Work\n",
            "\n",
            "\n",
            "If you liked The Joe Rogan Experience, try: \n",
            "     The Creative Penn Podcast For Writers\n",
            "     1001 Classic Short Stories & Tales\n",
            "     3 Books With Neil Pasricha\n",
            "     The Ground Up Show\n",
            "     1001 Stories For The Road\n",
            "     1001 Heroes, Legends, Histories & Mysteries Podcast\n",
            "\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "gKKeEQFg-ZXd",
        "outputId": "233b0e88-1d65-4082-aa38-29cd634c7f57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Try it yourself! \n",
        "your_podcast = \"Song Exploder\" #Replace this with a podcast of your choice!\n",
        "recs = recommend(your_podcast, cv_cosine_sim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS3unzgAao7w",
        "outputId": "e5807cb4-e617-477c-84fc-68ae6e7694ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you liked Song Exploder, try: \n",
            "     All Songs Considered\n",
            "     The Album Club\n",
            "     Celebration Rock\n",
            "     Song Confessional\n",
            "     And The Writer Is...with Ross Golan\n",
            "     The Sleeping At Last Podcast\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TFIDF + Cosine Similarity "
      ],
      "metadata": {
        "id": "7DyuFGZY-ZXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we use tf-idf to encode the podcast text and use that to generate a cosine similarity matrix."
      ],
      "metadata": {
        "id": "UJbzfA8vmE9k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tf = TfidfVectorizer()\n",
        "tf_matrix = tf.fit_transform(podcasts_df[\"text\"])\n",
        "tf_cosine_sim = cosine_similarity(tf_matrix)"
      ],
      "outputs": [],
      "metadata": {
        "id": "UvTkeTDr-ZXe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for i in sample_podcasts:\n",
        "    recs = recommend(i, tf_cosine_sim)\n",
        "    print('\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you liked The Daily, try: \n",
            "     Impeachment Inquiry: Updates from The Washington Post\n",
            "     The 11th Hour with Brian Williams\n",
            "     The Daily 202's Big Idea\n",
            "     Article II: Inside Impeachment\n",
            "     Impeachment: A Daily Podcast\n",
            "     The Takeaway\n",
            "\n",
            "\n",
            "If you liked Murder, etc., try: \n",
            "     Murder Minute\n",
            "     Criminology\n",
            "     Murderville\n",
            "     Unsolved Murders: True Crime Stories\n",
            "     Don't Talk to Strangers\n",
            "     True Crime All The Time Unsolved\n",
            "\n",
            "\n",
            "If you liked This American Life, try: \n",
            "     Experimental Brewing\n",
            "     1A\n",
            "     Through the Looking Glass: A LOST Retrospective\n",
            "     The Grave Talks | Haunted, Paranormal & Supernatural\n",
            "     Darkness Prevails Podcast | TRUE Horror Stories\n",
            "     BeerSmith Home and Beer Brewing Podcast\n",
            "\n",
            "\n",
            "If you liked Call Her Daddy, try: \n",
            "     hey, girl.\n",
            "     Girls Night with Stephanie May Wilson\n",
            "     Stiff Socks\n",
            "     Fierce Girls\n",
            "     Becoming Something with Jonathan Pokluda\n",
            "     Two Judgey Girls\n",
            "\n",
            "\n",
            "If you liked The Joe Rogan Experience, try: \n",
            "     MILLION DOLLAR LIFE LESSONS\n",
            "     Malcolm Gladwell, Revisionist History: Special Event\n",
            "     The Horror of Dolores Roach\n",
            "     Jordan Peterson Interviews & Speeches\n",
            "     Revisionist History\n",
            "     Ari Shaffir's Skeptic Tank\n",
            "\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "3xutGPIc-ZXe",
        "outputId": "e8a6db74-802d-4d6d-cb55-c67b05524921",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Try it yourself! \n",
        "your_podcast = \"Song Exploder\" #Replace this with a podcast of your choice!\n",
        "recs = recommend(your_podcast, tf_cosine_sim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBi5dWNibwYe",
        "outputId": "6667ff3d-8d40-4dc6-da1f-e51783f54cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you liked Song Exploder, try: \n",
            "     All Songs Considered\n",
            "     The Album Club\n",
            "     Celebration Rock\n",
            "     And The Writer Is...with Ross Golan\n",
            "     Song Confessional\n",
            "     Song Talk Radio | Songwriting Tips | Lyrics | Arranging | Live Feedback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare results of the two models"
      ],
      "metadata": {
        "id": "_lhHCXCH-ZXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to see whether or not the models tend to agree,\n",
        "and what amount of the total body of podcasts are ever actually recommended (do we solve the long tail problem)?"
      ],
      "metadata": {
        "id": "0rMAvkfQl59c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def print_compare(pod, num_recs=5):\n",
        "    \"\"\"for a given podcast and number of recommendations\n",
        "        print the recommendations from both tf-idf and cv\n",
        "        unique to tf-idf\n",
        "        and unique to cv\n",
        "    \"\"\"\n",
        "\n",
        "    tf_idf_recs = recommend(pod, tf_cosine_sim, num_recs, pretty_print=False)\n",
        "    cv_recs = recommend(pod, cv_cosine_sim, num_recs, pretty_print=False)\n",
        "\n",
        "    both = list(set(tf_idf_recs).intersection(set(cv_recs)))\n",
        "    unique_to_tf = list(set(tf_idf_recs).difference(set(cv_recs)))\n",
        "    unique_to_cv = list(set(cv_recs).difference(set(tf_idf_recs)))\n",
        "    print(\"Recs for {}: \".format(pod))\n",
        "    \n",
        "    print(\"    Recommended by both tf-idf and cv:\")\n",
        "    for i in both: print(\"         {}\".format(i))\n",
        "\n",
        "    print(\"    Uniqely recommended by tf-idf:\")\n",
        "    for i in unique_to_tf: print(\"         {}\".format(i))\n",
        "\n",
        "    print(\"    Uniqely recommended by cv:\")\n",
        "    for i in unique_to_cv: print(\"         {}\".format(i))\n",
        "    print('\\n')"
      ],
      "outputs": [],
      "metadata": {
        "id": "38Zdcc76-ZXg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for pod in sample_podcasts: print_compare(pod) "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recs for The Daily: \n",
            "    Recommended by both tf-idf and cv:\n",
            "         The 11th Hour with Brian Williams\n",
            "         Impeachment: A Daily Podcast\n",
            "         Article II: Inside Impeachment\n",
            "         Impeachment Inquiry: Updates from The Washington Post\n",
            "         The Takeaway\n",
            "         The Daily 202's Big Idea\n",
            "    Uniqely recommended by tf-idf:\n",
            "    Uniqely recommended by cv:\n",
            "\n",
            "\n",
            "Recs for Murder, etc.: \n",
            "    Recommended by both tf-idf and cv:\n",
            "         Don't Talk to Strangers\n",
            "         Murder Minute\n",
            "         Criminology\n",
            "         Murderville\n",
            "         True Crime All The Time Unsolved\n",
            "         Unsolved Murders: True Crime Stories\n",
            "    Uniqely recommended by tf-idf:\n",
            "    Uniqely recommended by cv:\n",
            "\n",
            "\n",
            "Recs for This American Life: \n",
            "    Recommended by both tf-idf and cv:\n",
            "    Uniqely recommended by tf-idf:\n",
            "         1A\n",
            "         The Grave Talks | Haunted, Paranormal & Supernatural\n",
            "         Darkness Prevails Podcast | TRUE Horror Stories\n",
            "         Experimental Brewing\n",
            "         BeerSmith Home and Beer Brewing Podcast\n",
            "         Through the Looking Glass: A LOST Retrospective\n",
            "    Uniqely recommended by cv:\n",
            "         This is the Gospel Podcast\n",
            "         Spooky Boo's Scary Story Time\n",
            "         The Story Home Children's Audio Stories\n",
            "         The Stoop Storytelling Series\n",
            "         1001 Heroes, Legends, Histories & Mysteries Podcast\n",
            "         The Story Behind\n",
            "\n",
            "\n",
            "Recs for Call Her Daddy: \n",
            "    Recommended by both tf-idf and cv:\n",
            "         Two Judgey Girls\n",
            "         Stiff Socks\n",
            "    Uniqely recommended by tf-idf:\n",
            "         hey, girl.\n",
            "         Becoming Something with Jonathan Pokluda\n",
            "         Fierce Girls\n",
            "         Girls Night with Stephanie May Wilson\n",
            "    Uniqely recommended by cv:\n",
            "         Hot Marriage. Cool Parents.\n",
            "         Safe For Work\n",
            "         NAKED with Catt Sadler\n",
            "         Slay Girl Slay\n",
            "\n",
            "\n",
            "Recs for The Joe Rogan Experience: \n",
            "    Recommended by both tf-idf and cv:\n",
            "    Uniqely recommended by tf-idf:\n",
            "         Jordan Peterson Interviews & Speeches\n",
            "         The Horror of Dolores Roach\n",
            "         MILLION DOLLAR LIFE LESSONS\n",
            "         Malcolm Gladwell, Revisionist History: Special Event\n",
            "         Revisionist History\n",
            "         Ari Shaffir's Skeptic Tank\n",
            "    Uniqely recommended by cv:\n",
            "         The Ground Up Show\n",
            "         1001 Stories For The Road\n",
            "         The Creative Penn Podcast For Writers\n",
            "         1001 Classic Short Stories & Tales\n",
            "         3 Books With Neil Pasricha\n",
            "         1001 Heroes, Legends, Histories & Mysteries Podcast\n",
            "\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "TnOhow7a-ZXh",
        "outputId": "f901679e-3472-4dc2-e208-d410ca557eac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try it yourself!\n",
        "\n",
        "your_podcast = \"Song Exploder\" #Replace this with your podcast \n",
        "print_compare(your_podcast)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfIav1MJhI_z",
        "outputId": "a093282d-634c-46bc-f710-c28f0649630b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recs for Song Exploder: \n",
            "    Recommended by both tf-idf and cv:\n",
            "         The Album Club\n",
            "         All Songs Considered\n",
            "         Celebration Rock\n",
            "         And The Writer Is...with Ross Golan\n",
            "         Song Confessional\n",
            "    Uniqely recommended by tf-idf:\n",
            "         Song Talk Radio | Songwriting Tips | Lyrics | Arranging | Live Feedback\n",
            "    Uniqely recommended by cv:\n",
            "         The Sleeping At Last Podcast\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def coverage(model_name, sim_matrix, num_recs=10):\n",
        "    \"\"\"Track what % of the overall library of podcasts\n",
        "        was ever actually recommended, when we serve\n",
        "        10 recs for each podcast in the library\n",
        "\n",
        "        parameters:\n",
        "          model_name: (str) either 'tf-idf' or 'cv'\n",
        "                    should correspond to the passed sim_matrix \n",
        "          sim_matrix: (np.array) an item-item similarity matrix\n",
        "          num_recs: how many recs for each item in library?\n",
        "        returns:\n",
        "          indices: (np.array) recommended podcast indices\n",
        "    \"\"\"\n",
        "    indices = np.argpartition(sim_matrix, -num_recs, axis=1)[:,-num_recs:]\n",
        "    \n",
        "    #calculating coverage:\n",
        "    recommended = set(list(itertools.chain(*indices)))\n",
        "    coverage = (len(recommended)/indices.shape[0])*100\n",
        "\n",
        "    print(\"Stats for {} Model with {} recs\".format(model_name, num_recs))\n",
        "    print(\"    Coverage: {} %\".format(coverage))\n",
        "    \n",
        "    return indices"
      ],
      "outputs": [],
      "metadata": {
        "id": "uQNrzYWT-ZXh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "cv_recs_10 = coverage(\"CountVectorizer\", cv_cosine_sim, 5)\n",
        "tf_idf_recs_10 = coverage(\"tf-idf\", tf_cosine_sim, 5)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stats for CountVectorizer Model with 5 recs\n",
            "    Coverage: 100.0 %\n",
            "Stats for tf-idf Model with 5 recs\n",
            "    Coverage: 100.0 %\n"
          ]
        }
      ],
      "metadata": {
        "id": "yuXMQRft-ZXh",
        "outputId": "4d760e24-c6b2-4743-fa58-1d40cfba2e95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Fake User Ratings"
      ],
      "metadata": {
        "id": "OXOK355G-ZXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to create users that have preferences.\n",
        "Each of them randomly rates between 5-20 randomly selected podcasts on a scale from 1-5. \n",
        "This is a non-realistic way to generate fake user ratings (as most users like similar things, and have a pattern to how they rate things). "
      ],
      "metadata": {
        "id": "S8T96RWnlP8t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def generate_user_ratings(users_count):\n",
        "    \"\"\"generates fake user ratings\n",
        "      parameters:\n",
        "        users_count: (int) how many fake users to generate\n",
        "      returns:\n",
        "        users: (pd.DataFrame) table of user, podcast, & rating    \n",
        "    \"\"\"\n",
        "    \n",
        "    user_ratings = []\n",
        "    for idx, user in enumerate(np.arange(0,users_count)):\n",
        "        ratings = []\n",
        "        quantity_rated = np.random.randint(5,21)\n",
        "        reviewed = set()\n",
        "        \n",
        "        for i in np.arange(quantity_rated):\n",
        "            podcast =  np.random.randint(0, podcasts_df.shape[0])\n",
        "            title = get_title_from_index(podcast)\n",
        "            \n",
        "            # don't want the same user to review \n",
        "            # the same podcast multiple times\n",
        "            while (podcast in reviewed):\n",
        "                podcast =  np.random.randint(0, podcasts_df.shape[0]+1)\n",
        "            reviewed.add(podcast)\n",
        "\n",
        "            rating = np.random.randint(1,6)\n",
        "            ratings.append([idx, podcast, rating, title])\n",
        "        \n",
        "        user_df = pd.DataFrame(ratings, \\\n",
        "                          columns=['user_id', 'podcast_idx', 'rating', 'podcast_title'])\n",
        "        user_ratings.append(user_df)\n",
        "    return pd.concat(user_ratings)"
      ],
      "outputs": [],
      "metadata": {
        "id": "4P4Z-47S-ZXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def checkUserProfile(user_idx, pretty_print=True):\n",
        "  \"\"\"For a given user id, create a profile including 3 attributes\n",
        "\n",
        "    parameters:\n",
        "      user_id: (int) user id\n",
        "      print: (boolean) whether or not printed outcomes are desired\n",
        "    \n",
        "    returns:\n",
        "      user_profile: (dict) contains 3 attributes of a user profile\n",
        "  \"\"\"\n",
        "  user_id=user_idx\n",
        "  user_reviews = usr.query('user_id==@user_id') \\\n",
        "          .sort_values('rating', ascending=False)\n",
        "  \n",
        "  user_profile = {'no_reviews' : user_reviews.shape[0], \n",
        "                  'top_5_shows' : user_reviews['podcast_title'].iloc[:5].to_list(), \n",
        "                  'ave_rating' : user_reviews['rating'].mean() }\n",
        "  \n",
        "  \n",
        "  #### formatting for pretty printing ###  \n",
        "  if pretty_print:\n",
        "    print(f\"User #{user_id} Profile:\")\n",
        "    print(f\"{user_profile['no_reviews']} reviews\")\n",
        "    print(f\"Average rating: {user_profile['ave_rating']} stars\")\n",
        "    print(f\"Top 5 shows:\")\n",
        "    \n",
        "    for show in user_profile['top_5_shows']:\n",
        "      print(f\"       {show}\")\n",
        "    print(\"                ..            \")\n",
        "  \n",
        "  return user_profile"
      ],
      "metadata": {
        "id": "nOGOKjsslPMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "num_users = 1000\n",
        "usr = generate_user_ratings(num_users)"
      ],
      "outputs": [],
      "metadata": {
        "id": "gQcI0Xdt-ZXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#investigate a random user!\n",
        "my_random_user = np.random.randint(0, num_users)\n",
        "profile = checkUserProfile(my_random_user)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0D8G8nLj2Jc",
        "outputId": "37a63746-003b-466c-8743-4fbbcc2bcd7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User #618 Profile:\n",
            "14 reviews\n",
            "Average rating: 3.0714285714285716 stars\n",
            "Top 5 shows:\n",
            "       The Vanished Podcast\n",
            "       Government Accountability Office (GAO) Podcast: Watchdog Report\n",
            "       Startup Hustle\n",
            "       Story Break\n",
            "       Middle:Below\n",
            "                ..            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement Collaborative Filtering"
      ],
      "metadata": {
        "id": "C-wfKWg9W-J1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our user rating data, we can implement collaborative filtering to generate recommendations based on user similarity. We specifically used the pyspark implementation of ALS Matrix Factorization with root mean squared error. \n",
        "\n",
        "We used a pyspark implementation of ALS code as published by [Jeffrey Chiang](https://github.com/chiang9/Medium_blog/blob/main/ALS_model/movielen%20ALS.ipynb)"
      ],
      "metadata": {
        "id": "_qOaLB11miZd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "sc = SparkContext.getOrCreate()\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark SQL basic example\") \\\n",
        "     \\\n",
        "    .getOrCreate()"
      ],
      "outputs": [],
      "metadata": {
        "id": "mYnRorb6-ZXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(usr)"
      ],
      "metadata": {
        "id": "YgApYMLUMsHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = df.randomSplit([0.7,0.3],111)"
      ],
      "metadata": {
        "id": "DThK-CVMKnT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we use the cross validator to tune the hyperparameters\n",
        "als = ALS(\n",
        "         userCol=\"user_id\", \n",
        "         itemCol=\"podcast_idx\",\n",
        "         ratingCol=\"rating\", \n",
        "         coldStartStrategy=\"drop\"\n",
        ")\n",
        "\n",
        "param_grid = ParamGridBuilder() \\\n",
        "            .addGrid(als.rank, [10, 100]) \\\n",
        "            .addGrid(als.regParam, [.1]) \\\n",
        "            .addGrid(als.maxIter, [10]) \\\n",
        "            .build()\n",
        "\n",
        "evaluator = RegressionEvaluator(\n",
        "           metricName=\"rmse\", \n",
        "           labelCol=\"rating\", \n",
        "           predictionCol=\"prediction\")\n",
        "\n",
        "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3, parallelism = 6)\n",
        "model = cv.fit(train)"
      ],
      "metadata": {
        "id": "LrTbKwJQK4Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = model.bestModel\n",
        "\n",
        "print(f\"Rank = {best_model._java_obj.parent().getRank()}\")\n",
        "print(f\"MaxIter = {best_model._java_obj.parent().getMaxIter()}\")\n",
        "print(f\"RegParam = {best_model._java_obj.parent().getRegParam()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2N3O2VjODsp",
        "outputId": "7750c611-ecfc-4cd6-ac8e-0f55ab3c418c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank = 100\n",
            "MaxIter = 10\n",
            "RegParam = 0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = best_model.transform(test)\n",
        "rmse = evaluator.evaluate(prediction)\n",
        "print(f'RMSE = {rmse}')\n",
        "\n",
        "# we can get the user latent factors and item latent factors from the model\n",
        "user_latent_features = best_model.userFactors\n",
        "item_latent_features = best_model.itemFactors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgdGDtPEOF8m",
        "outputId": "230bb3e1-64b3-464b-edf6-f38ee5dcf9eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE = 2.5798581166142873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_recs = best_model.recommendForAllUsers(3)\n",
        "user_recs_pandas = user_recs.toPandas()"
      ],
      "metadata": {
        "id": "3Fvq-IS6QF9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def checkUserRecommendations(user_row_idx, pretty_print=True):\n",
        "  \"\"\"Print each user's profile\n",
        "    and recommended future podcasts/predicted ratings\n",
        "\n",
        "    parameters:\n",
        "      user_row: (int) index of row from user_recs dataframe\n",
        "      print: (boolean) whether or not printed outcomes are desired\n",
        "    \n",
        "    returns:\n",
        "      user_recs: (list) recommended podcast titles\n",
        "  \"\"\"\n",
        "  user_row = user_recs_pandas.iloc[user_row_idx]\n",
        "  user_id = user_row['user_id']\n",
        "  user_profile = checkUserProfile(user_id)\n",
        "  \n",
        "  user_recs=[]\n",
        "  for rec in user_row['recommendations']:\n",
        "    rec_idx = rec.__getitem__('podcast_idx')\n",
        "    rec_title = get_title_from_index(rec_idx)\n",
        "    user_recs.append(rec_title)\n",
        "\n",
        "  #### formatting for pretty printing ###  \n",
        "  if pretty_print: \n",
        "    print(\"We recommend the following: \")\n",
        "    for rec_title in user_recs:\n",
        "      print(f\"       {rec_title}\")\n",
        "  print(\"\\n\")\n",
        "  return user_recs"
      ],
      "metadata": {
        "id": "iIeOnfSzQkjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking out the profiles & recommendations \n",
        "# for 10 random users\n",
        "\n",
        "for i in np.random.randint(0, len(user_recs_pandas), 10):\n",
        "  checkUserRecommendations(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO7xKR9xdzVM",
        "outputId": "1c6dec6e-8be4-4088-98bc-2c3373e510b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User #782 Profile:\n",
            "6 reviews\n",
            "Top 5 shows:\n",
            "       The Premed Years\n",
            "       The Official Average Boy Podcast\n",
            "       Appalachian Unsolved\n",
            "       Dr. Wayne W. Dyer Podcast\n",
            "       Teaching Hard History: American Slavery\n",
            "                ..            \n",
            "We recommend the following: \n",
            "       The Premed Years\n",
            "       Heartland Radio 2.0\n",
            "       The Sleeping At Last Podcast\n",
            "\n",
            "\n",
            "User #906 Profile:\n",
            "9 reviews\n",
            "Top 5 shows:\n",
            "       Guitar Music Theory\n",
            "       Informed Consent\n",
            "       Family Secrets\n",
            "       Ain't No Such Thing - Original Southern Horror Stories\n",
            "       Executive Edge\n",
            "                ..            \n",
            "We recommend the following: \n",
            "       Family Secrets\n",
            "       Informed Consent\n",
            "       The Week in Health Law\n",
            "\n",
            "\n",
            "User #459 Profile:\n",
            "7 reviews\n",
            "Top 5 shows:\n",
            "       Pregnancy Confidential\n",
            "       Medical Medium Podcast\n",
            "       The Rachel Maddow Show\n",
            "       DISGRACELAND\n",
            "       Sirenicide\n",
            "                ..            \n",
            "We recommend the following: \n",
            "       Pregnancy Confidential\n",
            "       Hang Up and Listen\n",
            "       Where Should We Begin? with Esther Perel\n",
            "\n",
            "\n",
            "User #347 Profile:\n",
            "14 reviews\n",
            "Top 5 shows:\n",
            "       That Classical Podcast\n",
            "       Til Death Do Us Blart\n",
            "       Recode Decode\n",
            "       Seincast: A Seinfeld Podcast\n",
            "       Unprofessional Engineering\n",
            "                ..            \n",
            "We recommend the following: \n",
            "       Recode Decode\n",
            "       Til Death Do Us Blart\n",
            "       That Classical Podcast\n",
            "\n",
            "\n",
            "User #211 Profile:\n",
            "8 reviews\n",
            "Top 5 shows:\n",
            "       Chips with everything podcast\n",
            "       Digital Planet\n",
            "       Morning Joe\n",
            "       Marx Madness\n",
            "       The Smoking Tire\n",
            "                ..            \n",
            "We recommend the following: \n",
            "       Morning Joe\n",
            "       Espionage\n",
            "       The Orbiting Human Circus\n",
            "\n",
            "\n",
            "User #864 Profile:\n",
            "5 reviews\n",
            "Top 5 shows:\n",
            "       Two Scientists Walk Into a Bar\n",
            "       Be Calm on Ahway Island Bedtime Stories\n",
            "       Clever\n",
            "       The Secret Life of Canada\n",
            "       Limited Resources\n",
            "                ..            \n",
            "We recommend the following: \n",
            "       Clever\n",
            "       Be Calm on Ahway Island Bedtime Stories\n",
            "       Dissect\n",
            "\n",
            "\n",
            "User #222 Profile:\n",
            "11 reviews\n",
            "Top 5 shows:\n",
            "       Team Beachbody Coach Podcast\n",
            "       Dark Myths\n",
            "       Radio Cherry Bombe\n",
            "       Trashy Divorces\n",
            "       Who Is?\n",
            "                ..            \n",
            "We recommend the following: \n",
            "       Dark Myths\n",
            "       Radio Cherry Bombe\n",
            "       Who Is?\n",
            "\n",
            "\n",
            "User #155 Profile:\n",
            "13 reviews\n",
            "Top 5 shows:\n",
            "       The Police Podcast\n",
            "       The Big Picture\n",
            "       Team Beachbody Coach Podcast\n",
            "       Made For This with Jennie Allen\n",
            "       Play Therapy Community\n",
            "                ..            \n",
            "We recommend the following: \n",
            "       The Big Picture\n",
            "       Made For This with Jennie Allen\n",
            "       The Police Podcast\n",
            "\n",
            "\n",
            "User #685 Profile:\n",
            "10 reviews\n",
            "Top 5 shows:\n",
            "       Trump, Inc.\n",
            "       The Anthropocene Reviewed\n",
            "       Quantum Physics I\n",
            "       Untangle\n",
            "       Deadly Manners\n",
            "                ..            \n",
            "We recommend the following: \n",
            "       The Anthropocene Reviewed\n",
            "       Quantum Physics I\n",
            "       Untangle\n",
            "\n",
            "\n",
            "User #1 Profile:\n",
            "11 reviews\n",
            "Top 5 shows:\n",
            "       Upzoned\n",
            "       Undiscovered\n",
            "       The ASES Podcast\n",
            "       Every Little Thing\n",
            "       The Chuck ToddCast: Meet the Press\n",
            "                ..            \n",
            "We recommend the following: \n",
            "       Undiscovered\n",
            "       The ASES Podcast\n",
            "       Kalila Stormfire's Economical Magick Services\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#try it yourself!\n",
        "my_random_user = np.random.randint(0, len(user_recs_pandas))\n",
        "recs = checkUserRecommendations(my_random_user)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJGewltVjWM2",
        "outputId": "ec06e55a-0536-407d-9456-91124d86248e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User #450 Profile:\n",
            "19 reviews\n",
            "Top 5 shows:\n",
            "       Joe Budden: Meet the Musician\n",
            "       Frankly Speaking About Family Medicine\n",
            "       Thinking Sideways Podcast\n",
            "       True Tales From Old Houses\n",
            "       Breaking Into Startups\n",
            "                ..            \n",
            "We recommend the following: \n",
            "       14 Days with Felicity\n",
            "       Noodle Loaf\n",
            "       Happy Face\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit ('base': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "interpreter": {
      "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}