{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import random\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import heapq\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcasts_df = pd.read_pickle('../data/pickle_files/english_podcasts_detailed_cleaned.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcasts_df['text'] = podcasts_df[['title', 'producer', 'genre', 'description', 'episode_titles', 'episode_descriptions']].apply(lambda x: ' '.join(x), axis=1)\n",
    "podcasts_df = podcasts_df.drop(columns=['genre', 'description', 'num_episodes', 'rating', 'num_reviews', 'link', 'episode_titles', 'episode_descriptions'])\n",
    "podcasts_df['ID'] = list(range(podcasts_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of stop words\n",
    "stop = get_stop_words('en')\n",
    "\n",
    "# remove non-alphanumeric, non-space\n",
    "stop = [re.sub(r'([^\\s\\w]|_)+', '', x) for x in stop]\n",
    "\n",
    "# add in custom stop words\n",
    "days = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "\n",
    "months = ['january', 'february', 'march', 'april', 'may', 'june', \n",
    "          'july', 'august', 'september', 'october', 'november', 'december']\n",
    "\n",
    "other = ['nan', 'podcast', 'podcasts', 'every', 'new', 'weekly', \n",
    "         'stories', 'story', 'episode', 'episodes', 'listen', \n",
    "         'host', 'hosted', 'join']\n",
    "\n",
    "[stop.append(str(day)) for day in days]\n",
    "[stop.append(str(month)) for month in months]\n",
    "[stop.append(str(x)) for x in other]\n",
    "\n",
    "def topKFrequent(tokenized_text, k): \n",
    "   \n",
    "    count = Counter(tokenized_text)   \n",
    "    \n",
    "    return heapq.nlargest(k, count.keys(), key=count.get)\n",
    "\n",
    "def remove_stop(text, stop):\n",
    "    custom_stop = stop\n",
    "#     top5 = topKFrequent(text, 5)\n",
    "#     custom_stop = custom_stop + top5\n",
    "    \n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        if word not in custom_stop:\n",
    "            new_text.append(word)\n",
    "    return new_text\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create stemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "l_stemmer = WordNetLemmatizer() \n",
    "\n",
    "\n",
    "def stem_list(text, p_stemmer):\n",
    "    new_list = []\n",
    "    for word in text:\n",
    "        new_list.append(p_stemmer.stem(word))\n",
    "    return new_list\n",
    "\n",
    "def lem_list(text, l_stemmer):\n",
    "    new_list = []\n",
    "    for word in text:\n",
    "        new_list.append(l_stemmer.lemmatize(word))\n",
    "    return new_list\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # remove mixed alphanumeric\n",
    "    text = re.sub(r\"\"\"(?x) # verbose regex\n",
    "                            \\b    # Start of word\n",
    "                            (?=   # Look ahead to ensure that this word contains...\n",
    "                             \\w*  # (after any number of alphanumeric characters)\n",
    "                             \\d   # ...at least one digit.\n",
    "                            )     # End of lookahead\n",
    "                            \\w+   # Match the alphanumeric word\n",
    "                            \\s*   # Match any following whitespace\"\"\", \n",
    "                             \"\", text)\n",
    "    \n",
    "    # remove urls (will check and remove http and www later)\n",
    "    text = re.sub(r'\\s([\\S]*.com[\\S]*)\\b', '', text)\n",
    "    text = re.sub(r'\\s([\\S]*.org[\\S]*)\\b', '', text)\n",
    "    text = re.sub(r'\\s([\\S]*.net[\\S]*)\\b', '', text)\n",
    "    text = re.sub(r'\\s([\\S]*.edu[\\S]*)\\b', '', text)\n",
    "    text = re.sub(r'\\s([\\S]*.gov[\\S]*)\\b', '', text)\n",
    "    \n",
    "    # remove non-alphanumeric, non-space\n",
    "    text = re.sub(r'([^\\s\\w]|_)+', '', text)\n",
    "    \n",
    "    # tokenize text\n",
    "    text = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    # remove stop words\n",
    "    text = remove_stop(text, stop)\n",
    "    \n",
    "    # stem\n",
    "    text = lem_list(text, l_stemmer)\n",
    "    \n",
    "    # remove instances of http or www\n",
    "    new_text_list = []\n",
    "    for word in text:\n",
    "        if re.search(r'http', word):\n",
    "            continue\n",
    "        if re.search(r'www', word):\n",
    "            continue\n",
    "        new_text_list.append(word)\n",
    "    \n",
    "    new_text = ' '.join(new_text_list)\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcasts_df['text'] = podcasts_df['text'].map(preprocess_text)\n",
    "podcasts_df = podcasts_df[podcasts_df.text != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>producer</th>\n",
       "      <th>text</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>History Hyenas with Chris Distefano and Yannis...</td>\n",
       "      <td>RiotCast Network</td>\n",
       "      <td>history hyena chris distefano yannis pappa rio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Curiosity Daily</td>\n",
       "      <td>Westwood One</td>\n",
       "      <td>curiosity daily westwood one education awardwi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spirits</td>\n",
       "      <td>Multitude</td>\n",
       "      <td>spirit multitude history boozy mythology legen...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Soundtrack Show</td>\n",
       "      <td>iHeartRadio</td>\n",
       "      <td>soundtrack show iheartradio tv film soundtrack...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Writing Excuses</td>\n",
       "      <td>Brandon Sanderson, Mary Robinette Kowal, Dan W...</td>\n",
       "      <td>writing excuse brandon sanderson mary kowal da...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  History Hyenas with Chris Distefano and Yannis...   \n",
       "1                                    Curiosity Daily   \n",
       "2                                            Spirits   \n",
       "3                                The Soundtrack Show   \n",
       "4                                    Writing Excuses   \n",
       "\n",
       "                                            producer  \\\n",
       "0                                   RiotCast Network   \n",
       "1                                       Westwood One   \n",
       "2                                          Multitude   \n",
       "3                                        iHeartRadio   \n",
       "4  Brandon Sanderson, Mary Robinette Kowal, Dan W...   \n",
       "\n",
       "                                                text  ID  \n",
       "0  history hyena chris distefano yannis pappa rio...   0  \n",
       "1  curiosity daily westwood one education awardwi...   1  \n",
       "2  spirit multitude history boozy mythology legen...   2  \n",
       "3  soundtrack show iheartradio tv film soundtrack...   3  \n",
       "4  writing excuse brandon sanderson mary kowal da...   4  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcasts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podcast Recommendation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def get_title_from_index(index):\n",
    "    return podcasts_df[podcasts_df.ID == index][\"title\"].values[0]\n",
    "\n",
    "def get_index_from_title(title):\n",
    "    return podcasts_df[podcasts_df.title == title][\"ID\"].values[0]\n",
    "\n",
    "def get_recommendations(podcast_id, sim_matrix):\n",
    "    recommendations = list()\n",
    "    \n",
    "    podcast_title = get_title_from_index(podcast_id)\n",
    "    similar_podcasts =  list(enumerate(sim_matrix[podcast_id]))\n",
    "    sorted_similar_podcast = sorted(similar_podcasts,key=lambda x:x[1],reverse=True)\n",
    "    \n",
    "    for i in range(11):\n",
    "        title = get_title_from_index(sorted_similar_podcast[i][0])\n",
    "        recommendations.append(title)\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "test_podcasts = ['The Daily', 'Up First', 'VIEWS with David Dobrik and Jason Nash', 'Impaulsive with Logan Paul',\n",
    "                 'The Bill Simmons Podcast', 'My Favorite Murder with Karen Kilgariff and Georgia Hardstark',\n",
    "                 'This American Life', 'Joel Osteen Podcast', 'TED Radio Hour', 'Call Her Daddy', \n",
    "                 'Skip and Shannon: Undisputed'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity + CountVectorizer (Bag of Words) Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv_matrix = cv.fit_transform(podcasts_df[\"text\"])\n",
    "cv_cosine_sim = cosine_similarity(cv_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Daily', 'Impeachment Inquiry: Updates from The Washington Post', 'Impeachment: A Daily Podcast', 'The Takeaway', 'Article II: Inside Impeachment', \"The Daily 202's Big Idea\", 'The 11th Hour with Brian Williams', 'Bill O’Reilly’s No Spin News and Analysis', 'The Last Word with Lawrence O’Donnell', 'Up First', 'Impeachment Today']\n",
      "\n",
      "['Up First', 'Impeachment: A Daily Podcast', 'Impeachment Inquiry: Updates from The Washington Post', 'The Daily', 'Article II: Inside Impeachment', 'The 11th Hour with Brian Williams', 'Bill O’Reilly’s No Spin News and Analysis', \"The Daily 202's Big Idea\", 'The Takeaway', 'Can He Do That?', 'The Last Word with Lawrence O’Donnell']\n",
      "\n",
      "['VIEWS with David Dobrik and Jason Nash', 'Instant Message', 'I Am In Eskew', 'Jalen & Jacoby', 'The Axe Files with David Axelrod', 'Getting Things Done', 'The Tower', 'The Permaculture Podcast', 'Making It With Jimmy Diresta, Bob Clagett and David Picciuto', 'Psychology of Eating', 'Blank Check with Griffin & David']\n",
      "\n",
      "['Impaulsive with Logan Paul', 'Sexology', 'BEHIND THE SCENES', 'MOONFACE', 'The Tom Woods Show', 'The Flop House', 'The Long Run with Luke Timmerman', 'Heartland Radio 2.0', 'Another Kingdom', 'Curious with Josh Peck', 'Just Roll With It - A Dungeons and Dragons Podcast']\n",
      "\n",
      "['The Bill Simmons Podcast', 'The Rewatchables', 'Book of Basketball 2.0', 'Real Time with Bill Maher', 'The Ringer NFL Show', 'NFL: The Dave Dameshek Football Program', 'Monday Morning Podcast', 'Chris Simms Unbuttoned', 'The Ryen Russillo Podcast', 'The Bill Bert Podcast', 'The GM Shuffle with Michael Lombardi and Adnan Virk']\n",
      "\n",
      "['My Favorite Murder with Karen Kilgariff and Georgia Hardstark', \"Don't Talk to Strangers\", 'Murder Minute', 'Wine & Crime', 'Fresh Hell Podcast', 'Unsolved Murders: True Crime Stories', 'Murder, Myth & Mystery', 'Criminology', 'Jensen and Holes: The Murder Squad', 'Murder, etc.', 'Murder In The Rain']\n",
      "\n",
      "['This American Life', 'History of Japan', 'ED MYLETT SHOW', 'The Toxic People Detox | Self-Care & Difficult People Survival Strategies', 'Meaningful Conversations with Maria Shriver', 'This is the Gospel Podcast', 'The Hidden People', 'Locked Up Abroad', 'For The Love With Jen Hatmaker Podcast', 'The Incomparable Radio Theater', 'The Stacking Benjamins Show']\n",
      "\n",
      "['Joel Osteen Podcast', 'Joel Osteen Podcast', 'Saddleback Church Weekend Messages', 'Daily Grace', 'Marriage After God', 'Love Worth Finding on Oneplace.com', 'All Things Catholic with Dr. Edward Sri', 'Bethel Church Sermon of the Week', 'The Porch', 'Rush: Holy Spirit in Modern Life | A Practical & Prophetic Podcast for Men and Women', \"Don't Mom Alone Podcast\"]\n",
      "\n",
      "['TED Radio Hour', 'The Metaphysical Hour hosted by Julia Cannon', 'TED Talks Art', 'TED Talks Business', 'CarProUSA Radio Show', 'TED Talks Science and Medicine', 'The TED Interview', 'Freakonomics Radio', 'The Creeping Hour', 'TED Talks Society and Culture', 'Zero Hours']\n",
      "\n",
      "['Call Her Daddy', 'Stiff Socks', 'Two Judgey Girls', 'NAKED with Catt Sadler', 'Becoming Something with Jonathan Pokluda', 'Slay Girl Slay', 'Hot Marriage. Cool Parents.', 'Safe For Work', 'The Walynn Collins Show', 'Coffee Convos Podcast with Kail Lowry & Lindsie Chrisley', 'Girl Were You Alone? An *NSYNC Podcast']\n",
      "\n",
      "['Skip and Shannon: Undisputed', 'First Things First', 'Golic and Wingo', 'Speak For Yourself with Whitlock & Wiley', 'The Herd with Colin Cowherd', 'The Odd Couple with Chris Broussard & Rob Parker', 'The Michael Kay Show', 'Joe Benigno and Evan Roberts', 'Pick Six NFL Podcast', 'Pardon My Take', 'The Ringer NFL Show']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in test_podcasts:\n",
    "    print(get_recommendations(get_index_from_title(i), cv_cosine_sim))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity + TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "tf_matrix = tf.fit_transform(podcasts_df[\"text\"])\n",
    "tf_cosine_sim = cosine_similarity(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Daily', 'Impeachment Inquiry: Updates from The Washington Post', \"The Daily 202's Big Idea\", 'The 11th Hour with Brian Williams', 'Article II: Inside Impeachment', 'Impeachment: A Daily Podcast', 'The Takeaway', 'The Last Word with Lawrence O’Donnell', 'Bill O’Reilly’s No Spin News and Analysis', 'The Situation Room with Wolf Blitzer', 'The Rachel Maddow Show']\n",
      "\n",
      "['Up First', 'Impeachment Inquiry: Updates from The Washington Post', 'The Daily', 'Article II: Inside Impeachment', 'The 11th Hour with Brian Williams', 'Impeachment: A Daily Podcast', \"The Daily 202's Big Idea\", 'Can He Do That?', 'The Last Word with Lawrence O’Donnell', 'Bill O’Reilly’s No Spin News and Analysis', 'The Takeaway']\n",
      "\n",
      "['VIEWS with David Dobrik and Jason Nash', 'Instant Message', 'How Did This Get Made?', 'Light The Fight- Parenting Podcast', 'Flow Sessions with Jason Silva', 'This Week in Startups - Audio', 'The Tower', 'True Cold Case Files with Jason and Daisy', 'Horror Hill: A Horror Anthology and Scary Stories Series Podcast', 'Kids Bible Stories', 'Psychology of Eating']\n",
      "\n",
      "['Impaulsive with Logan Paul', 'MOONFACE', 'The Tom Woods Show', 'Sexology', 'The Tim Dillon Show', 'The Long Run with Luke Timmerman', 'Ripper Magoos with Bob Menery', 'Come Follow Me Daily', 'The Flop House', 'Sex Talk With My Mom', 'Below the Belt']\n",
      "\n",
      "['The Bill Simmons Podcast', 'The Rewatchables', 'Book of Basketball 2.0', 'Headlong: Missing Richard Simmons', 'The Ringer NFL Show', 'Real Time with Bill Maher', 'Chris Simms Unbuttoned', 'Speak For Yourself with Whitlock & Wiley', 'The Pat McAfee Show 2.0', 'NFL: The Dave Dameshek Football Program', 'The GM Shuffle with Michael Lombardi and Adnan Virk']\n",
      "\n",
      "['My Favorite Murder with Karen Kilgariff and Georgia Hardstark', 'Do You Need A Ride?', 'Wire Talk with Karen Stubbs', 'The Murder In My Family', 'Murder Minute', 'The Walking Dead ‘Cast', 'Murderous Minors: killer kids', \"Don't Talk to Strangers\", 'Murder, Myth & Mystery', 'Jensen and Holes: The Murder Squad', 'Criminology']\n",
      "\n",
      "['This American Life', 'Experimental Brewing', '1A', 'Through the Looking Glass: A LOST Retrospective', 'BeerSmith Home and Beer Brewing Podcast', 'Talking Beat - from the Portland Police Bureau', 'Haunted Hell House of Horror', 'The Grave Talks | Haunted, Paranormal & Supernatural', 'Sinisterhood', 'Slow Burn', 'Haunted Places']\n",
      "\n",
      "['Joel Osteen Podcast', 'Joel Osteen Podcast', 'Joyce Meyer Ministries TV Podcast', 'Joyce Meyer Enjoying Everyday Life® TV Audio Podcast', 'Marriage After God', 'Daily Grace', 'Fresh Life Church', 'Brilliant Perspectives', 'Bethel Church Sermon of the Week', 'Saddleback Church Weekend Messages', 'Pastor Robert Morris Ministries on Oneplace.com']\n",
      "\n",
      "['TED Radio Hour', 'The TED Interview', 'TED Talks Art', 'TED Talks Science and Medicine', 'TED Talks Business', \"Hunted: Inside Ted Bundy's Trail of Terror\", 'TED Talks Society and Culture', 'TED Talks Education', 'TED Talks Daily (HD video)', 'TED Talks Daily (SD video)', 'TED Talks Music']\n",
      "\n",
      "['Call Her Daddy', 'hey, girl.', 'Becoming Something with Jonathan Pokluda', 'Fierce Girls', 'Stiff Socks', 'Girls Night with Stephanie May Wilson', 'Grammar Girl Quick and Dirty Tips for Better Writing', 'Two Judgey Girls', 'The Clever Girls Know Podcast', 'Quarter Life Crisis', 'Slay Girl Slay']\n",
      "\n",
      "['Skip and Shannon: Undisputed', 'First Things First', 'Golic and Wingo', 'Speak For Yourself with Whitlock & Wiley', 'The Herd with Colin Cowherd', 'The Odd Couple with Chris Broussard & Rob Parker', 'Pick Six NFL Podcast', 'First Take', 'Pardon My Take', 'Chris Simms Unbuttoned', 'Pro Football Talk Live with Mike Florio']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in test_podcasts:\n",
    "    print(get_recommendations(get_index_from_title(i), tf_cosine_sim))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity + Custom Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_X = []\n",
    "        for document in X:\n",
    "            tokenized_doc = []\n",
    "            for sent in nltk.sent_tokenize(document):\n",
    "                tokenized_doc += nltk.word_tokenize(sent)\n",
    "            transformed_X.append(np.array(tokenized_doc))\n",
    "        return np.array(transformed_X)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.wv.vectors[0])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = MyTokenizer().fit_transform(X)\n",
    "        \n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = list(podcasts_df.text)\n",
    "tokenized_text = [tokenizer.tokenize(i) for i in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(tokenized_text, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.save_word2vec_format('../word2vec/customembedding.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-6dbee262e849>:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(transformed_X)\n"
     ]
    }
   ],
   "source": [
    "mean_embedding_vectorizer = MeanEmbeddingVectorizer(w2v_model)\n",
    "mean_embedded = mean_embedding_vectorizer.fit_transform(podcasts_df['text'])\n",
    "w2v_cosine_sim = cosine_similarity(mean_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Daily', 'Impeachment: A Daily Podcast', 'Up First', 'Can He Do That?', 'The Takeaway', 'Mark Levin Podcast', 'Mark Levin Podcast', 'Global News Podcast', 'Post Reports', 'Skimm This', 'Rubicon: The Impeachment of Donald Trump']\n",
      "\n",
      "['Up First', 'The Daily', 'Impeachment: A Daily Podcast', 'The Takeaway', 'Can He Do That?', 'Post Reports', 'Mark Levin Podcast', 'Mark Levin Podcast', 'Global News Podcast', 'Bill O’Reilly’s No Spin News and Analysis', 'Talking Feds']\n",
      "\n",
      "['VIEWS with David Dobrik and Jason Nash', 'Amy Schumer Presents: 3 Girls, 1 Keith', 'The Fighter & The Kid', 'Ringer Dish', 'Not Skinny But Not Fat', 'Andrea Savage: A Grown-Up Woman #buttholes', 'Drew and Mike Show', 'The Three Questions with Andy Richter', 'The Archers', 'Skotcast with Jeff Wittek & Scotty Sire', 'Fitzdog Radio']\n",
      "\n",
      "['Impaulsive with Logan Paul', 'Drew and Mike Show', 'Comments by Celebs', 'Heartland Radio 2.0', 'Who? Weekly', 'KFC Radio', 'Ya Neva Know: you know what I mean?', 'Curious with Josh Peck', 'Coffee Convos Podcast with Kail Lowry & Lindsie Chrisley', 'To L And Back: An L Word Podcast', 'Berning In Hell']\n",
      "\n",
      "['The Bill Simmons Podcast', 'The Ryen Russillo Podcast', 'ESPN Podcasts', 'The Peter King Podcast', 'Pardon My Take', 'Scal and Pals', 'The Woj Pod', 'NFL: Move the Sticks with Daniel Jeremiah & Bucky Brooks', 'The Jump', 'The Ringer NBA Show', 'The Mina Kimes Show featuring Lenny']\n",
      "\n",
      "['My Favorite Murder with Karen Kilgariff and Georgia Hardstark', 'Wine & Crime', 'Fresh Hell Podcast', 'Murderous Minors: killer kids', 'Jensen and Holes: The Murder Squad', 'Cult Liter with Spencer Henry', \"Don't Talk to Strangers\", 'Housewives of True Crime', 'Murder, Myth & Mystery', \"Let's Taco 'Bout True Crime\", 'Last Podcast On The Left']\n",
      "\n",
      "['This American Life', 'Radio Diaries', 'The Incomparable Radio Theater', 'Locked Up Abroad', 'Undiscovered', 'Radiolab', 'Travel with Rick Steves', 'Brought to you by...', 'Our Strange Skies', 'Code Switch', 'Mobituaries with Mo Rocca']\n",
      "\n",
      "['Joel Osteen Podcast', 'Joel Osteen Podcast', 'Saddleback Church Weekend Messages', 'VOUS Church', 'Daily Grace', 'The Porch', 'The Fr. Mike Schmitz Catholic Podcast', 'North Point Community Church', 'Rush: Holy Spirit in Modern Life | A Practical & Prophetic Podcast for Men and Women', 'Made For This with Jennie Allen', 'Glorious in the Mundane Podcast with Christy Nockels']\n",
      "\n",
      "['TED Radio Hour', 'The goop Podcast', 'TED Talks Society and Culture', 'WGRL NYC', 'Experts on Expert with Dax Shepard', 'Deepak Chopra’s Infinite Potential', 'The After On Podcast', 'The Kevin Rose Show', 'The Permaculture Podcast', 'Green Dreamer: Sustainability and Regeneration From Ideas to Life', 'The TED Interview']\n",
      "\n",
      "['Call Her Daddy', 'Stiff Socks', 'Zane and Heath: Unfiltered', 'Bully and the Beast', 'Pretty Basic with Alisha Marie and Remi Cruz', 'Coffee Convos Podcast with Kail Lowry & Lindsie Chrisley', 'Berning In Hell', 'Schnitt Talk', 'Woman Evolve with Sarah Jakes Roberts', 'Again With This: Beverly Hills, 90210 & Melrose Place', 'DELETE THIS']\n",
      "\n",
      "['Skip and Shannon: Undisputed', 'First Things First', 'High Noon', 'Jalen & Jacoby', 'Speak For Yourself with Whitlock & Wiley', 'Golic and Wingo', 'The Herd with Colin Cowherd', 'First Take', 'PTI', 'ESPN Podcasts', 'Pro Football Talk Live with Mike Florio']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in test_podcasts:\n",
    "    print(get_recommendations(get_index_from_title(i), w2v_cosine_sim))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity + GloVe Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../word2vec/glove.6B.50d.txt.word2vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-160b3e86b101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mglove_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../word2vec/glove.6B.50d.txt.word2vec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \"\"\"\n\u001b[0;32m-> 1723\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1724\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2051\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2052\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2053\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2054\u001b[0m             \u001b[0;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../word2vec/glove.6B.50d.txt.word2vec'"
     ]
    }
   ],
   "source": [
    "glove_model = KeyedVectors.load_word2vec_format(\"../word2vec/glove.6B.50d.txt.word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glove_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-d200e87a2b12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mglove_mean_embedding_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeanEmbeddingVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mglove_mean_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglove_mean_embedding_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpodcasts_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mglove_cosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_mean_embedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glove_model' is not defined"
     ]
    }
   ],
   "source": [
    "glove_mean_embedding_vectorizer = MeanEmbeddingVectorizer(glove_model)\n",
    "glove_mean_embedded = glove_mean_embedding_vectorizer.fit_transform(podcasts_df['text'])\n",
    "glove_cosine_sim = cosine_similarity(glove_mean_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Daily', 'Up First', 'Impeachment: A Daily Podcast', 'The Lawfare Podcast', 'Bag Man', 'Can He Do That?', 'CBS This Morning', 'Impeachment Inquiry: Updates from The Washington Post', 'All In with Chris Hayes', 'The Takeaway', 'The New Yorker: Politics and More']\n",
      "\n",
      "['Up First', 'The Daily', 'The Lawfare Podcast', 'Impeachment: A Daily Podcast', 'Bag Man', 'What Next | Daily News and Analysis', 'Impeachment Inquiry: Updates from The Washington Post', 'The Situation Room with Wolf Blitzer', 'PBS NewsHour - Segments', 'All In with Chris Hayes', 'The Lead with Jake Tapper']\n",
      "\n",
      "['VIEWS with David Dobrik and Jason Nash', 'Help! I Suck at Dating with Dean, Vanessa and Jared', 'Amy Schumer Presents: 3 Girls, 1 Keith', 'Dead Pilots Society', \"So Bad It's Good with Ryan Bailey\", 'The Ben and Ashley I Almost Famous Podcast', 'Out in the Wild', 'Your 2 Dads w/ Sean & Julian', 'Good Morning From Hell', 'The Bobby Bones Show', 'Scrubbing In with Becca Tilley & Tanya Rad']\n",
      "\n",
      "['Impaulsive with Logan Paul', 'The Kirk Minihane Show', 'Reality Steve Podcast', 'Hollywood Unlocked [UNCENSORED]', 'Two Judgey Girls', 'The HoneyDew with Ryan Sickler', 'The Last Laugh: A Daily Beast Podcast', 'The Adam and Dr. Drew Show', 'QAnon Anonymous', 'Hot Marriage. Cool Parents.', 'The Candace Owens Show']\n",
      "\n",
      "['The Bill Simmons Podcast', 'The Peter King Podcast', 'First Things First', 'ESPN Podcasts', 'NFL: Move the Sticks with Daniel Jeremiah & Bucky Brooks', 'Cleveland Browns Daily & More', 'Joe Benigno and Evan Roberts', 'Book of Basketball 2.0', 'Chris Simms Unbuttoned', 'Footballguys.com - The Audible - Fantasy Football Info for Serious Fans', 'The Herd with Colin Cowherd']\n",
      "\n",
      "['My Favorite Murder with Karen Kilgariff and Georgia Hardstark', 'Murderous Minors: killer kids', 'You Must Remember Manson', '...These Are Their Stories: The Law & Order Podcast', 'Fresh Hell Podcast', \"Let's Taco 'Bout True Crime\", 'Hollywood & Crime', 'Atlanta Monster', 'Red Scare', 'Reality Life with Kate Casey', 'Once Upon A Crime | True Crime']\n",
      "\n",
      "['This American Life', 'UK True Crime Podcast', 'A Waste Of Time with ItsTheReal', \"Heaven's Gate\", 'Locked Up Abroad', 'The David Banner Podcast', 'The Brooklyn Blast Furnace', 'The Incomparable Radio Theater', 'Astonishing Legends', 'Outside Podcast', 'Upzoned']\n",
      "\n",
      "['Joel Osteen Podcast', 'Joel Osteen Podcast', 'North Point Community Church', 'VOUS Church', 'Pathway to Victory on Oneplace.com', 'Passion City Church Podcast', 'The Fr. Mike Schmitz Catholic Podcast', 'As For Me And My House', '4:13 Podcast', 'All In', 'Daily Grace']\n",
      "\n",
      "['TED Radio Hour', 'Under The Skin with Russell Brand', 'Creative Processing with Joseph Gordon-Levitt', 'Love Your Voice with Roger Love', 'The Ezra Klein Show', 'The Kevin Rose Show', 'Ten Percent Happier with Dan Harris', 'Conversations with Tyler', 'Being Well with Dr. Rick Hanson', 'Freakonomics Radio', 'The Enneagram Journey']\n",
      "\n",
      "['Call Her Daddy', 'A Date With Dateline', \"Couldn't Help But Wonder: A Sex and the City Podcast with Jamie Lee and Rose Surnow\", 'Derek Sivers', \"The On-Call Room: A Grey's Anatomy Podcast\", 'Get Off My Lawn Podcast w/ Gavin McInnes', 'The Papaya Podcast', 'The Podfathers', 'DELETE THIS', 'Crime in Sports', 'Take it or Leave it']\n",
      "\n",
      "['Skip and Shannon: Undisputed', 'First Things First', 'Golic and Wingo', 'Speak For Yourself with Whitlock & Wiley', 'Joe Benigno and Evan Roberts', 'The Odd Couple with Chris Broussard & Rob Parker', 'Scal and Pals', 'The Lowe Post', 'The Herd with Colin Cowherd', 'The Bill Simmons Podcast', 'Jalen & Jacoby']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in test_podcasts:\n",
    "    print(get_recommendations(get_index_from_title(i), glove_cosine_sim))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity + Word2Vec + Smooth Inverse Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_first_principal_component(X):\n",
    "    svd = TruncatedSVD(n_components=1, n_iter=7, random_state=0)\n",
    "    svd.fit(X)\n",
    "    pc = svd.components_\n",
    "    XX = X - X.dot(pc.transpose()) * pc\n",
    "    return XX\n",
    "\n",
    "def smooth_inverse_frequency(sent, a=0.001, word2vec_model=w2v_model):\n",
    "    word_counter = {}\n",
    "    sentences = []\n",
    "    total_count = 0\n",
    "    no_of_sentences = 0\n",
    "    \n",
    "    for s in sent:\n",
    "        for w in s:\n",
    "            if w in word_counter:\n",
    "                word_counter[w] = word_counter[w] + 1\n",
    "            else:\n",
    "                word_counter[w] = 1\n",
    "        total_count = total_count + len(s)\n",
    "        no_of_sentences = no_of_sentences + 1\n",
    "    \n",
    "    sents_emd = []\n",
    "    for s in sent:\n",
    "        sent_emd = []\n",
    "        for word in s:\n",
    "            if word in word2vec_model.wv:\n",
    "                emd = (a/(a + (word_counter[word]/total_count)))*word2vec_model[word]\n",
    "                sent_emd.append(emd)\n",
    "        sum_ = np.array(sent_emd).sum(axis=0)\n",
    "        sentence_emd = sum_/float(no_of_sentences)\n",
    "        sents_emd.append(sentence_emd)\n",
    "        \n",
    "    new_sents_emb = remove_first_principal_component(np.array(sents_emd))\n",
    "    return new_sents_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Word2Vec' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-d29720f20bca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msif_text_emd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_inverse_frequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msif_cosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msif_text_emd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-cb11f9b01b09>\u001b[0m in \u001b[0;36msmooth_inverse_frequency\u001b[0;34m(sent, a, word2vec_model)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword2vec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0memd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mword2vec_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0msent_emd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0msum_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_emd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Word2Vec' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "sif_text_emd = smooth_inverse_frequency(text_list)\n",
    "sif_cosine_sim = cosine_similarity(sif_text_emd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Daily', 'Impeachment Inquiry: Updates from The Washington Post', 'The Drum Show', \"Official Prime Minister's Questions (PMQs) Podcast\", 'Criminal', 'Robb Wolf - The Paleo Solution Podcast - Paleo diet, nutrition, fitness, and health', 'Queens Podcast', 'Aquarium Co-Op Podcast', 'The Beauty Brains', 'Story Time', 'Plenary Session']\n",
      "\n",
      "['Up First', 'The Paul Tripp Podcast', \"The Daily 202's Big Idea\", 'Stay Tuned with Preet', 'The General Hospital Podcast', 'Post Reports', 'Article II: Inside Impeachment', 'Impeachment Inquiry: Updates from The Washington Post', 'Intelligence Squared', 'WGRL NYC', 'The Drum Show']\n",
      "\n",
      "['VIEWS with David Dobrik and Jason Nash', 'The Man With A Thousand Faces', 'Solid Joys Daily Devotional', 'The Axe Files with David Axelrod', \"DJ Private Ryan's Podcast\", 'Beach Too Sandy, Water Too Wet', 'Kid Friendly Joke Of The Day', 'AI: Hype vs. Reality', 'Just The Sip', 'Tumble Science Podcast for Kids', 'Binge Mode: Star Wars']\n",
      "\n",
      "['Impaulsive with Logan Paul', 'Leveling Up: Creating Everything From Nothing with Natalie Jill', 'The Tom Ferry Podcast Experience', 'PAVE: Professionals Against Violence Podcast', 'The Trail Went Cold', 'The Overwhelmed Brain', 'The American War', 'The Fraudcast', 'All In The Mind - ABC RN', 'Airplane Geeks Podcast', 'Building a StoryBrand with Donald Miller']\n",
      "\n",
      "['The Bill Simmons Podcast', 'Bible Stories My Kids Love', 'Bloomberg Businessweek', 'The Mandolins and Beer Podcast', 'College Basketball Talk on NBC Sports Podcast', 'The GM Shuffle with Michael Lombardi and Adnan Virk', 'Not Skinny But Not Fat', 'The Michael Brooks Show', 'Footballguys.com - The Audible - Fantasy Football Info for Serious Fans', 'The CONAN Podcast', 'The Glass Appeal']\n",
      "\n",
      "['My Favorite Murder with Karen Kilgariff and Georgia Hardstark', 'Food Talk with Dani Nierenberg', 'Dave & Chuck the Freak Podcast', 'The Lefkoe Show', 'Deck The Hallmark', 'Car Talk', 'Call Your Girlfriend', 'Rock N Roll Manifesto (mp3)', 'Through the Wire', 'Mindful Mama - Parenting with Mindfulness', 'Nightmare On Film Street - A Horror Movie Podcast']\n",
      "\n",
      "['This American Life', 'The Bridge', 'RULES FOR RETROGRADES', 'Bizarre Albums', 'Mysterious Radio: Paranormal, UFO & Lore Interviews', 'Surprisingly Awesome', 'Witness History: World War 2 Collection', 'Hey Riddle Riddle', 'Twenty Thousand Hertz', 'Haunted Hell House of Horror', 'Hard Factor']\n",
      "\n",
      "['Joel Osteen Podcast', 'Joel Osteen Podcast', \"WHOA That's Good Podcast\", 'My Dad Wrote A Porno', 'Sibling Revelry with Kate Hudson and Oliver Hudson', 'Start Today Morning Show', 'Astonishing Legends', 'That Sounds Fun with Annie F. Downs', 'The Endless Honeymoon Podcast', 'MarriageToday Audio Podcast', 'Detective']\n",
      "\n",
      "['TED Radio Hour', 'The History of Exploration', 'How to Be a Girl', 'Sincerely, X: Season 1', 'The Research Like a Pro Genealogy Podcast', 'ONE Extraordinary Marriage Show', 'Shameless Sex', 'Bedtime Explorers', 'Elon Musk Interviews', 'Deep Energy 2.0 - Music for Sleep, Meditation, Relaxation, Massage and Yoga', 'StartUp Podcast']\n",
      "\n",
      "['Call Her Daddy', 'Clapcast from Claptone', 'The Martin Garrix Show', 'Frame Trap', 'W&W Rave Culture Radio', 'Nora En Pure - Purified Radio', 'Box-Trick: A Retro Gaming Podcast', 'Perfecto Podcast: featuring Paul Oakenfold', 'Read-Aloud Revival', 'X-Men: The Audio Drama', 'The Box of Oddities']\n",
      "\n",
      "['Skip and Shannon: Undisputed', 'All The Smoke', 'Pull Up with CJ McCollum', 'Chicks in the Office', 'The Sports Junkies', 'The Great Books', 'To L And Back: An L Word Podcast', 'Pete and Sebastian Show', 'SModcast', \"Van Lathan's The Red Pill\", 'Comedy Bang Bang: The Podcast']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in test_podcasts:\n",
    "    print(get_recommendations(get_index_from_title(i), sif_cosine_sim))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
